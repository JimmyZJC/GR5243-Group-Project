{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GR5243 Group Project\n",
    "##### Xingchen Ji, Yuting Wang, Hongyi Xu, and Jiacan Zhou"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = pd.read_csv(\"../Data/RTA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) Invalid Variable Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta.drop([\"Service_year_of_vehicle\", \"Defect_of_vehicle\", \"Work_of_casuality\", \"Fitness_of_casuality\"], axis = 1, inplace = True)\n",
    "eta.drop([\"Time\", \"Weather_conditions\", \"Casualty_class\", \"Sex_of_casualty\", \"Age_band_of_casualty\", \"Casualty_severity\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [col for col in eta.columns]\n",
    "categorical.remove(\"Number_of_vehicles_involved\")\n",
    "categorical.remove(\"Number_of_casualties\")\n",
    "categorical.remove(\"Accident_severity\")\n",
    "numerical = [\"Number_of_vehicles_involved\", \"Number_of_casualties\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b) Missing Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta.dropna(subset = categorical, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Explanatory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta[\"Accident_severity\"].value_counts()\n",
    "fig, ax = plt.subplots(1, 2, figsize = (15, 5))\n",
    "eta[\"Accident_severity\"].value_counts().plot.pie(ax = ax[0], autopct = \"%.2f%%\", title = \"Pie Chart of Accident Severity\")\n",
    "sns.histplot(eta[\"Accident_severity\"], ax = ax[1]).set(title = \"Histogram of Accident Severity\", xlabel = \"Accident Severity\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b) Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = \"Number_of_vehicles_involved\", y = \"Number_of_casualties\", data = eta)\n",
    "plt.title(\"Scatterplot of Number of Vehicles Involved vs Number of Casualties\")\n",
    "plt.xlabel(\"Number of Vehicles Involved\")\n",
    "plt.ylabel(\"Number of Casualties\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(7, 3, figsize = (30, 90))\n",
    "for var, subplot in zip(categorical, ax.flatten()):\n",
    "    sns.histplot(eta[var], ax = subplot).set(ylabel = \"\")\n",
    "    subplot.set_title(\"Histogram of \" + var, fontsize = 18)\n",
    "    for label in subplot.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "    if len(var) > 5:\n",
    "        subplot.set_xlabel(var, fontsize = 6)\n",
    "    else:\n",
    "        subplot.set_xlabel(var, fontsize = 12)\n",
    "    if subplot != ax[-1, -1]:\n",
    "        subplot.set_xlabel(\"\")\n",
    "plt.subplots_adjust(wspace = 0.2, hspace = 0.4)\n",
    "fig.delaxes(ax[-1, -1])\n",
    "fig.delaxes(ax[-1, -2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (c) Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = eta.corr()\n",
    "plt.figure(figsize = (9, 6))\n",
    "sns.heatmap(corr, annot = True, cmap = \"coolwarm\").set(title = \"Correlation Matrix\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4. Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "eta_fs = eta.copy()\n",
    "eta_fs[numerical] = scaler.fit_transform(eta_fs[numerical])\n",
    "y_cgan = eta_fs[\"Accident_severity\"]\n",
    "\n",
    "y_cgan = pd.get_dummies(y_cgan, columns = [\"Accident_severity\"])\n",
    "y = y_cgan.to_numpy(dtype = np.float32)\n",
    "X_cgan = pd.get_dummies(eta_fs, columns = categorical)\n",
    "X_cgan = X_cgan.drop(\"Accident_severity\", axis = 1)\n",
    "X = X_cgan.to_numpy(dtype = np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b) Data Augmentation by CGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CGAN_on_Scratch import *\n",
    "dataset = TensorDataset(torch.tensor(X, dtype = torch.float32), torch.tensor(y, dtype = torch.float32))\n",
    "dataloader = DataLoader(dataset, batch_size = 512, shuffle = True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "noise_dim = 100\n",
    "label_dim = y_cgan.shape[1]\n",
    "input_dim = X_cgan.shape[1]\n",
    "num_numerical = 2\n",
    "num_categorical = 156\n",
    "best_params = bayesian_optimization_cgan(dataloader, device, noise_dim, label_dim, num_numerical, num_categorical)\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(noise_dim, label_dim, num_numerical, num_categorical, best_params[\"gen_hidden_dim\"]).to(device)\n",
    "discriminator = Discriminator(label_dim, num_numerical, num_categorical, best_params[\"disc_hidden_dim\"]).to(device)\n",
    "generator.apply(he_init)\n",
    "discriminator.apply(he_init)\n",
    "gen_optimizer = optim.Adam(generator.parameters(), lr = best_params[\"gen_lr\"], betas = (best_params[\"gen_beta1\"], 0.999))\n",
    "disc_optimizer = optim.Adam(discriminator.parameters(), lr = best_params[\"disc_lr\"], betas = (best_params[\"disc_beta1\"], 0.999))\n",
    "train_cgan(generator, discriminator, dataloader, device, gen_optimizer, disc_optimizer, best_params[\"num_epochs\"], noise_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_slight_injury_data(generator, num_data, noise_dim, label_dim, device):\n",
    "    noise = torch.randn(num_data, noise_dim).to(device)\n",
    "    labels = torch.zeros(num_data, label_dim).to(device)\n",
    "    labels[:, 2] = 1\n",
    "    fake_data = generator(noise, labels)\n",
    "    return fake_data.detach().cpu()\n",
    "\n",
    "def generate_serious_injury_data(generator, num_data, noise_dim, label_dim, device):\n",
    "    noise = torch.randn(num_data, noise_dim).to(device)\n",
    "    labels = torch.zeros(num_data, label_dim).to(device)\n",
    "    labels[:, 1] = 1\n",
    "    fake_data = generator(noise, labels)\n",
    "    return fake_data.detach().cpu()\n",
    "\n",
    "def generate_fatal_injury_data(generator, num_data, noise_dim, label_dim, device):\n",
    "    noise = torch.randn(num_data, noise_dim).to(device)\n",
    "    labels = torch.zeros(num_data, label_dim).to(device)\n",
    "    labels[:, 0] = 1\n",
    "    fake_data = generator(noise, labels)\n",
    "    return fake_data.detach().cpu()\n",
    "\n",
    "def round_one_hot(encoded_data, num_categories):\n",
    "    start_index = 0\n",
    "    for cate in num_categories:\n",
    "        current_data = encoded_data[:, start_index:start_index + cate]\n",
    "        max_indices = torch.argmax(current_data, dim = 1, keepdim = True)\n",
    "        one_hot = torch.zeros_like(encoded_data)\n",
    "        one_hot.scatter_(1, max_indices, 1)\n",
    "        encoded_data[:, start_index:start_index + cate] = one_hot[:, start_index:start_index + cate]\n",
    "        start_index += cate\n",
    "    return encoded_data\n",
    "\n",
    "num_categories = [7, 5, 3, 7, 4, 7, 17, 4, 13, 7, 9, 8, 5, 4, 4, 10, 13, 9, 20]\n",
    "fake1_data = generate_serious_injury_data(generator, 4000, noise_dim, label_dim, device)\n",
    "fake1_numerical = fake1_data[:, :num_numerical]\n",
    "fake1_categorical = fake1_data[:, num_numerical:]\n",
    "fake1_categorical = round_one_hot(fake1_categorical, num_categories)\n",
    "fake1_data = np.concatenate((fake1_numerical, fake1_categorical), axis = 1)\n",
    "fake1_data = pd.DataFrame(fake1_data, columns = X_cgan.columns)\n",
    "\n",
    "fake2_data = generate_fatal_injury_data(generator, 3000, noise_dim, label_dim, device)\n",
    "fake2_numerical = fake2_data[:, :num_numerical]\n",
    "fake2_categorical = fake2_data[:, num_numerical:]\n",
    "fake2_categorical = round_one_hot(fake2_categorical, num_categories)\n",
    "fake2_data = np.concatenate((fake2_numerical, fake2_categorical), axis = 1)\n",
    "fake2_data = pd.DataFrame(fake2_data, columns = X_cgan.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake1_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake2_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug = pd.concat([X_cgan, fake1_data, fake2_data], axis = 0).reset_index(drop = True)\n",
    "y_fake = np.zeros((7000, 3))\n",
    "y_fake[:4000, 1] = 1\n",
    "y_fake[4000:, 0] = 1\n",
    "y_fake = pd.DataFrame(y_fake, columns = y_cgan.columns)\n",
    "y_aug = pd.concat([y_cgan, y_fake], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_aug.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (c) Dimensionality Reduction by PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_aug)\n",
    "pca_data = pca.transform(X_aug)\n",
    "pca_data_var = pca.explained_variance_ratio_\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(np.cumsum(pca_data_var), marker = \"o\", linestyle = \"--\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.xlim(0, 100)\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"Explained Variance vs Number of Components\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 60\n",
    "pca_denoise = PCA(n_components = threshold)\n",
    "pca_denoise.fit(X_aug)\n",
    "data_pca_denoised = pca_denoise.transform(X_aug)\n",
    "X_denoised = pca_denoise.inverse_transform(data_pca_denoised)\n",
    "X_denoised = pd.DataFrame(X_denoised, columns = X_aug.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (d) Feature Selection by XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_xg = y_aug.values\n",
    "y_xg = np.argmax(y_xg, axis = 1)\n",
    "y_xg = pd.Series(y_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_denoised, y_xg, test_size = 0.3, random_state = 233)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(n_estimators = 100, max_depth = 3, learning_rate = 0.1, random_state = 233)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "feature_importance = xgb.feature_importances_\n",
    "feature_importance = pd.DataFrame(feature_importance, index = X_train.columns, columns = [\"importance\"]).sort_values(\"importance\", ascending = False)\n",
    "aggregated_features = {}\n",
    "variable_index = categorical + numerical\n",
    "for feature in variable_index:\n",
    "    for i in range(len(feature_importance.index)):\n",
    "        if feature in feature_importance.index[i]:\n",
    "            if feature in aggregated_features:\n",
    "                aggregated_features[feature] += feature_importance.iloc[i, 0]\n",
    "            else:\n",
    "                aggregated_features[feature] = feature_importance.iloc[i, 0]\n",
    "        else:\n",
    "            if feature not in aggregated_features:\n",
    "                aggregated_features[feature] = 0\n",
    "aggregated_features = pd.DataFrame.from_dict(aggregated_features, orient = \"index\", columns = [\"importance\"]).sort_values(\"importance\", ascending = False)\n",
    "aggregated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "sns.barplot(x = aggregated_features[\"importance\"], y = aggregated_features.index).set(title = \"Feature Importance\", xlabel = \"Importance\", ylabel = \"Features\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = aggregated_features[aggregated_features[\"importance\"] < 0.01].index.tolist()\n",
    "drop_list = []\n",
    "for feature in drop_features:\n",
    "    for i in range(len(X_denoised.columns)):\n",
    "        if feature in X_denoised.columns[i]:\n",
    "            drop_list.append(X_denoised.columns[i])\n",
    "X_denoised.drop(drop_list, axis = 1, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5. Data Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = X_denoised.copy()\n",
    "y_final = y_aug.copy()\n",
    "y_final = y_final.values\n",
    "y_final = np.argmax(y_final, axis = 1)\n",
    "y_final = pd.Series(y_final)\n",
    "X_final.to_csv(\"X_final.csv\", index = False)\n",
    "y_final.to_csv(\"y_final.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"X_final.csv\")\n",
    "y = pd.read_csv(\"y_final.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 233)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6. Model Selection\n",
    "#### (a) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "param_svc = {\"C\": [0.1, 1, 10, 100], \"gamma\": [1, 0.1, 0.01, 0.001], \"kernel\": [\"rbf\", \"linear\", \"poly\"], \"degree\": [1, 2, 3, 4]}\n",
    "svc = GridSearchCV(SVC(), param_svc, refit = True, verbose = 0, n_jobs = 12)\n",
    "svc.fit(X_train, y_train)\n",
    "print(svc.best_params_)\n",
    "print(svc.best_estimator_)\n",
    "y_pred = svc.predict(X_test)\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification report: \", classification_report(y_test, y_pred))\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(mat.T, square = True, annot = True, fmt = \"d\", cbar = False, xticklabels = [\"Fatal\", \"Serious\", \"Slight\"], yticklabels = [\"Fatal\", \"Serious\", \"Slight\"])\n",
    "plt.xlabel(\"True Label\")\n",
    "plt.ylabel(\"Predicted Label\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "param_gbc = {\"learning_rate\": [0.1, 0.01, 0.001], \"n_estimators\": [100, 200, 300], \"max_depth\": [1, 2, 3, 4, 5]}\n",
    "gbc = GridSearchCV(GradientBoostingClassifier(), param_gbc, refit = True, verbose = 0, n_jobs = 12)\n",
    "gbc.fit(X_train, y_train)\n",
    "print(gbc.best_params_)\n",
    "print(gbc.best_estimator_)\n",
    "y_pred = gbc.predict(X_test)\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification report: \", classification_report(y_test, y_pred))\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(mat.T, square = True, annot = True, fmt = \"d\", cbar = False, xticklabels = [\"Fatal\", \"Serious\", \"Slight\"], yticklabels = [\"Fatal\", \"Serious\", \"Slight\"])\n",
    "plt.xlabel(\"True Label\")\n",
    "plt.ylabel(\"Predicted Label\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "param_mlp = {\"hidden_layer_sizes\": [(128, 128), (128, 128, 128),(128, 128, 128, 128), (128, 128, 128, 128, 128)], \"activation\": [\"relu\", \"logistic\"], \"solver\": \"adam\", \"alpha\": [0.001, 0.01, 0.05, 0.1], \"learning_rate\": [\"constant\", \"adaptive\"]}\n",
    "mlp = GridSearchCV(MLPClassifier(), param_mlp, refit = True, verbose = 0, n_jobs = 12)\n",
    "mlp.fit(X_train, y_train)\n",
    "print(mlp.best_params_)\n",
    "print(mlp.best_estimator_)\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification report: \", classification_report(y_test, y_pred))\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(mat.T, square = True, annot = True, fmt = \"d\", cbar = False, xticklabels = [\"Fatal\", \"Serious\", \"Slight\"], yticklabels = [\"Fatal\", \"Serious\", \"Slight\"])\n",
    "plt.xlabel(\"True label\")\n",
    "plt.ylabel(\"Predicted label\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "    \"Model\": [\"Support Vector Machine\", \"Gradient Boosting\", \"Multilayer Perceptron\"],\n",
    "    \"Accuracy\": [accuracy_score(y_test, svc.predict(X_test)), accuracy_score(y_test, gbc.predict(X_test)), accuracy_score(y_test, mlp.predict(X_test))]})\n",
    "models.sort_values(by = \"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x = \"Model\", y = \"Accuracy\", data = models.sort_values(by = \"Accuracy\", ascending = False))\n",
    "for acc in ax.containers:\n",
    "    ax.bar_label(acc, label_type = \"center\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-mine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e3d6c2fd17c985616d208c207ad01a559fd73b1923d8aab6e801588cb03416b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
